{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UP/DOWN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Betting(gym.Env):\n",
    "\n",
    "    # actions available\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(Betting, self).__init__() # gym.Env의 __init__ 호출\n",
    "\n",
    "        # data 정의\n",
    "        self.data = data\n",
    "        self.size = len(data) # size of the data\n",
    "        self.range = 1000  # range of the data\n",
    "\n",
    "        # randomly assign the inital location of agent\n",
    "        self.observe_idx = np.random.randint(self.size - 1)\n",
    "        self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        # respective actions of agents : up, down\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # set the observation space to (1,) to represent agent position\n",
    "        self.observation_space = spaces.Box(low=0, high=self.range, shape=(1,9,), dtype=np.uint16)\n",
    "\n",
    "    def step(self, action):\n",
    "        info = {}  # additional information\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # UP, DOWN 맞으면 reward=1, 틀리면 맞을 때까지 반복\n",
    "        if action == self.UP:\n",
    "            if 1000 < self.data[self.observe_idx + 1][1]:\n",
    "                reward += 1\n",
    "                self.observe_idx += 1\n",
    "            else:\n",
    "                reward += 0\n",
    "        elif action == self.DOWN:\n",
    "            if 1000 > self.data[self.observe_idx + 1][1]:\n",
    "                reward += 1\n",
    "                self.observe_idx += 1\n",
    "            else:\n",
    "                reward += 0\n",
    " #       else:\n",
    " #           raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "        # 더 이상 데이터가 없을 경우, done\n",
    "        done = bool(self.observe_idx == self.size - 1)\n",
    "        \n",
    "        if not done:\n",
    "            self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        return np.array([self.agent_position]).astype(np.uint8), reward, done, info\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        '''\n",
    "            render the state\n",
    "        '''\n",
    "#         if mode != 'console':\n",
    "#             raise NotImplementedError()\n",
    "\n",
    "#         for pos in range(self.size):\n",
    "#             if pos == self.agent_position:\n",
    "#                 print(\"X\", end='')\n",
    "#             else:\n",
    "#                 print('.', end='')\n",
    "#             print('')\n",
    "\n",
    "    def reset(self):\n",
    "        # -1 to ensure agent inital position will not be at the end state\n",
    "        self.observe_idx = np.random.randint(self.size - 2)\n",
    "        self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        return np.array([self.agent_position]).astype(np.uint8)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/env_checker.py:202: UserWarning: Your observation has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\"Your observation has an unconventional shape (neither an image, nor a 1D vector). \"\n"
     ]
    }
   ],
   "source": [
    "# This is to test if custom enviroment created properly\n",
    "# If the environment don't follow the gym interface, an error will be thrown\n",
    "\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "data = [[1000,1000,1004,986,1000,1004,973,988,1235],\n",
    "   [1000,1002,1012,997,1000,1010,986,1178,1495],\n",
    "   [1000,979,1000,978,1000,989,969,1008,1222],\n",
    "   [1000,952,1012,951,1000,951,927,653,750],\n",
    "   [1000,1002,1016,993,1000,954,923,660,665],\n",
    "   [1000,1006,1006,987,1000,973,935,918,830]]\n",
    "\n",
    "env = Betting(data)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# try draw the grid world\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:176: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:298: The name tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:546: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:548: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:221: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:306: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:307: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:914: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/kfac.py:621: The name tf.self_adjoint_eig is deprecated. Please use tf.linalg.eigh instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jwk/anaconda3/envs/tf115/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py:319: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "---------------------------------\n",
      "| explained_variance | 0.0706   |\n",
      "| fps                | 47       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| policy_loss        | 1.22     |\n",
      "| total_timesteps    | 20       |\n",
      "| value_loss         | 3.74     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.712    |\n",
      "| fps                | 1308     |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.648    |\n",
      "| policy_loss        | 0.153    |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 0.853    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.876    |\n",
      "| fps                | 1540     |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.4      |\n",
      "| policy_loss        | 0.0874   |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 0.0575   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.603    |\n",
      "| fps                | 1628     |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.638    |\n",
      "| policy_loss        | -0.121   |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 0.35     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -1.19e-07 |\n",
      "| fps                | 1674      |\n",
      "| nupdates           | 400       |\n",
      "| policy_entropy     | 0.135     |\n",
      "| policy_loss        | -0.00491  |\n",
      "| total_timesteps    | 8000      |\n",
      "| value_loss         | 0.0176    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.397    |\n",
      "| fps                | 1704     |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.328    |\n",
      "| policy_loss        | 0.336    |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 1.98     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 1724     |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.136    |\n",
      "| policy_loss        | -0.00467 |\n",
      "| total_timesteps    | 12000    |\n",
      "| value_loss         | 0.0147   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.842    |\n",
      "| fps                | 1740     |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.402    |\n",
      "| policy_loss        | -0.0574  |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 0.0755   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | 0.252    |\n",
      "| fps                | 1756     |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.39     |\n",
      "| policy_loss        | -0.466   |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 1.78     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.409    |\n",
      "| fps                | 1768     |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.48     |\n",
      "| policy_loss        | 0.0202   |\n",
      "| total_timesteps    | 18000    |\n",
      "| value_loss         | 1.27     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.748    |\n",
      "| fps                | 1780     |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.515    |\n",
      "| policy_loss        | 0.0638   |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 0.261    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.671    |\n",
      "| fps                | 1786     |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.567    |\n",
      "| policy_loss        | 0.0831   |\n",
      "| total_timesteps    | 22000    |\n",
      "| value_loss         | 0.374    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.804    |\n",
      "| fps                | 1792     |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.153    |\n",
      "| policy_loss        | -0.0883  |\n",
      "| total_timesteps    | 24000    |\n",
      "| value_loss         | 0.0685   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.771    |\n",
      "| fps                | 1797     |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.558    |\n",
      "| policy_loss        | -0.306   |\n",
      "| total_timesteps    | 26000    |\n",
      "| value_loss         | 0.46     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.977    |\n",
      "| fps                | 1804     |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.589    |\n",
      "| policy_loss        | 0.059    |\n",
      "| total_timesteps    | 28000    |\n",
      "| value_loss         | 0.0347   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.953    |\n",
      "| fps                | 1810     |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 0.492    |\n",
      "| policy_loss        | -0.05    |\n",
      "| total_timesteps    | 30000    |\n",
      "| value_loss         | 0.047    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.654    |\n",
      "| fps                | 1814     |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 0.484    |\n",
      "| policy_loss        | -0.0998  |\n",
      "| total_timesteps    | 32000    |\n",
      "| value_loss         | 0.28     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.805    |\n",
      "| fps                | 1818     |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.58     |\n",
      "| policy_loss        | -0.116   |\n",
      "| total_timesteps    | 34000    |\n",
      "| value_loss         | 0.265    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.745    |\n",
      "| fps                | 1820     |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.614    |\n",
      "| policy_loss        | -0.188   |\n",
      "| total_timesteps    | 36000    |\n",
      "| value_loss         | 0.357    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.833    |\n",
      "| fps                | 1824     |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.571    |\n",
      "| policy_loss        | -0.064   |\n",
      "| total_timesteps    | 38000    |\n",
      "| value_loss         | 0.265    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.931    |\n",
      "| fps                | 1827     |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.408    |\n",
      "| policy_loss        | -0.0283  |\n",
      "| total_timesteps    | 40000    |\n",
      "| value_loss         | 0.0955   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.842    |\n",
      "| fps                | 1830     |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.455    |\n",
      "| policy_loss        | -0.114   |\n",
      "| total_timesteps    | 42000    |\n",
      "| value_loss         | 0.192    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.737    |\n",
      "| fps                | 1831     |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 0.522    |\n",
      "| policy_loss        | 0.148    |\n",
      "| total_timesteps    | 44000    |\n",
      "| value_loss         | 0.667    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.829    |\n",
      "| fps                | 1831     |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 0.562    |\n",
      "| policy_loss        | -0.0969  |\n",
      "| total_timesteps    | 46000    |\n",
      "| value_loss         | 0.212    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.788    |\n",
      "| fps                | 1829     |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.443    |\n",
      "| policy_loss        | -0.0107  |\n",
      "| total_timesteps    | 48000    |\n",
      "| value_loss         | 0.345    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.687    |\n",
      "| fps                | 1830     |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 0.511    |\n",
      "| policy_loss        | -0.186   |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.217    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.872    |\n",
      "| fps                | 1832     |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 0.622    |\n",
      "| policy_loss        | -0.149   |\n",
      "| total_timesteps    | 52000    |\n",
      "| value_loss         | 0.24     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.819    |\n",
      "| fps                | 1833     |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.556    |\n",
      "| policy_loss        | -0.0374  |\n",
      "| total_timesteps    | 54000    |\n",
      "| value_loss         | 0.293    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.815    |\n",
      "| fps                | 1834     |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.553    |\n",
      "| policy_loss        | -0.0506  |\n",
      "| total_timesteps    | 56000    |\n",
      "| value_loss         | 0.259    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.518    |\n",
      "| fps                | 1836     |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.56     |\n",
      "| policy_loss        | -0.149   |\n",
      "| total_timesteps    | 58000    |\n",
      "| value_loss         | 0.338    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.791    |\n",
      "| fps                | 1839     |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.589    |\n",
      "| policy_loss        | -0.0778  |\n",
      "| total_timesteps    | 60000    |\n",
      "| value_loss         | 0.347    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.759    |\n",
      "| fps                | 1841     |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.581    |\n",
      "| policy_loss        | -0.0966  |\n",
      "| total_timesteps    | 62000    |\n",
      "| value_loss         | 0.301    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.868    |\n",
      "| fps                | 1844     |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.557    |\n",
      "| policy_loss        | -0.105   |\n",
      "| total_timesteps    | 64000    |\n",
      "| value_loss         | 0.161    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.764    |\n",
      "| fps                | 1846     |\n",
      "| nupdates           | 3300     |\n",
      "| policy_entropy     | 0.514    |\n",
      "| policy_loss        | -0.116   |\n",
      "| total_timesteps    | 66000    |\n",
      "| value_loss         | 0.286    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.781    |\n",
      "| fps                | 1848     |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 0.592    |\n",
      "| policy_loss        | -0.132   |\n",
      "| total_timesteps    | 68000    |\n",
      "| value_loss         | 0.355    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | 0.844    |\n",
      "| fps                | 1849     |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 0.599    |\n",
      "| policy_loss        | 0.0181   |\n",
      "| total_timesteps    | 70000    |\n",
      "| value_loss         | 0.112    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.849    |\n",
      "| fps                | 1850     |\n",
      "| nupdates           | 3600     |\n",
      "| policy_entropy     | 0.564    |\n",
      "| policy_loss        | -0.0532  |\n",
      "| total_timesteps    | 72000    |\n",
      "| value_loss         | 0.282    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.825    |\n",
      "| fps                | 1852     |\n",
      "| nupdates           | 3700     |\n",
      "| policy_entropy     | 0.606    |\n",
      "| policy_loss        | -0.0343  |\n",
      "| total_timesteps    | 74000    |\n",
      "| value_loss         | 0.224    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.908    |\n",
      "| fps                | 1854     |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.619    |\n",
      "| policy_loss        | -0.205   |\n",
      "| total_timesteps    | 76000    |\n",
      "| value_loss         | 0.174    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.655    |\n",
      "| fps                | 1856     |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.626    |\n",
      "| policy_loss        | 0.064    |\n",
      "| total_timesteps    | 78000    |\n",
      "| value_loss         | 0.453    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.809    |\n",
      "| fps                | 1857     |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.62     |\n",
      "| policy_loss        | -0.148   |\n",
      "| total_timesteps    | 80000    |\n",
      "| value_loss         | 0.312    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.769    |\n",
      "| fps                | 1859     |\n",
      "| nupdates           | 4100     |\n",
      "| policy_entropy     | 0.592    |\n",
      "| policy_loss        | 0.0121   |\n",
      "| total_timesteps    | 82000    |\n",
      "| value_loss         | 0.326    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.881    |\n",
      "| fps                | 1860     |\n",
      "| nupdates           | 4200     |\n",
      "| policy_entropy     | 0.579    |\n",
      "| policy_loss        | -0.137   |\n",
      "| total_timesteps    | 84000    |\n",
      "| value_loss         | 0.128    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.882    |\n",
      "| fps                | 1861     |\n",
      "| nupdates           | 4300     |\n",
      "| policy_entropy     | 0.552    |\n",
      "| policy_loss        | -0.163   |\n",
      "| total_timesteps    | 86000    |\n",
      "| value_loss         | 0.154    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.905    |\n",
      "| fps                | 1862     |\n",
      "| nupdates           | 4400     |\n",
      "| policy_entropy     | 0.583    |\n",
      "| policy_loss        | -0.0383  |\n",
      "| total_timesteps    | 88000    |\n",
      "| value_loss         | 0.14     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.705    |\n",
      "| fps                | 1863     |\n",
      "| nupdates           | 4500     |\n",
      "| policy_entropy     | 0.52     |\n",
      "| policy_loss        | -0.0462  |\n",
      "| total_timesteps    | 90000    |\n",
      "| value_loss         | 0.42     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.741    |\n",
      "| fps                | 1865     |\n",
      "| nupdates           | 4600     |\n",
      "| policy_entropy     | 0.578    |\n",
      "| policy_loss        | 0.255    |\n",
      "| total_timesteps    | 92000    |\n",
      "| value_loss         | 0.688    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.759    |\n",
      "| fps                | 1866     |\n",
      "| nupdates           | 4700     |\n",
      "| policy_entropy     | 0.591    |\n",
      "| policy_loss        | 0.0268   |\n",
      "| total_timesteps    | 94000    |\n",
      "| value_loss         | 0.438    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.734    |\n",
      "| fps                | 1867     |\n",
      "| nupdates           | 4800     |\n",
      "| policy_entropy     | 0.619    |\n",
      "| policy_loss        | 0.141    |\n",
      "| total_timesteps    | 96000    |\n",
      "| value_loss         | 0.516    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.795    |\n",
      "| fps                | 1868     |\n",
      "| nupdates           | 4900     |\n",
      "| policy_entropy     | 0.62     |\n",
      "| policy_loss        | 0.0988   |\n",
      "| total_timesteps    | 98000    |\n",
      "| value_loss         | 0.394    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.816    |\n",
      "| fps                | 1869     |\n",
      "| nupdates           | 5000     |\n",
      "| policy_entropy     | 0.553    |\n",
      "| policy_loss        | -0.191   |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 0.197    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import various RL algorithms\n",
    "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
    "\n",
    "# Train the agent\n",
    "model = ACKTR('MlpPolicy', env, verbose=1).learn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 1 done= False\n",
      "Step 2\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 3\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 4\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 5\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 6\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 7\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 8\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 9\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 10\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 11\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 12\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 13\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 14\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 15\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 16\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 17\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 18\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 19\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 20\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 21\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 22\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 23\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 24\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 25\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 26\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 27\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 28\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 29\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 30\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 31\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 32\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 33\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 34\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 35\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 36\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 37\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 38\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 39\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 40\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 41\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 42\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 43\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 44\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 45\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 46\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 47\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 48\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 49\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n",
      "Step 50\n",
      "Action:  0\n",
      "obs= [[232 234 244 229 232 242 218 154 215]] reward= 0 done= False\n"
     ]
    }
   ],
   "source": [
    "# running the simulation with trained model to verify result\n",
    "\n",
    "obs = env.reset()\n",
    "n_steps = 50\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 prob :  [0.8182154  0.18178457]\n"
     ]
    }
   ],
   "source": [
    "# print the trained policy map\n",
    "# recall UP = 0, DOWN = 1\n",
    "obs = [[1000,1002,1012,997,1000,1010,986,1178,1495]]\n",
    "action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "print(\"action:\", action,\"prob : \", model.action_probability(obs), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf115",
   "language": "python",
   "name": "tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
