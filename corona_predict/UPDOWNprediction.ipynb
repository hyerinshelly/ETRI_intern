{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UP/DOWN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Betting(gym.Env):\n",
    "\n",
    "    # actions available\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(Betting, self).__init__() # gym.Env의 __init__ 호출\n",
    "\n",
    "        # data 정의\n",
    "        self.data = data\n",
    "        self.size = len(data)  # size of the data\n",
    "\n",
    "        # randomly assign the inital location of agent\n",
    "        self.observe_idx = np.random.randint(self.size - 1)\n",
    "        self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        # respective actions of agents : up, down\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # set the observation space to (1,) to represent agent position\n",
    "        self.observation_space = spaces.Box(low=0, high=self.size, shape=(1,), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        info = {}  # additional information\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # UP, DOWN 맞으면 reward=1, 틀리면 맞을 때까지 반복\n",
    "        if action == self.UP:\n",
    "            if self.data[self.observe_idx] < self.data[self.observe_idx + 1]:\n",
    "                reward += 1\n",
    "                self.observe_idx += 1\n",
    "            else:\n",
    "                reward += 0\n",
    "        elif action == self.DOWN:\n",
    "            if self.data[self.observe_idx] > self.data[self.observe_idx + 1]:\n",
    "                reward += 1\n",
    "                self.observe_idx += 1\n",
    "            else:\n",
    "                reward += 0\n",
    "        else:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "        # 더 이상 데이터가 없을 경우, done\n",
    "        done = bool(self.observe_idx == self.size - 1)\n",
    "        \n",
    "        if not done:\n",
    "            self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        return np.array([self.agent_position]).astype(np.uint8), reward, done, info\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        '''\n",
    "            render the state\n",
    "        '''\n",
    "#         if mode != 'console':\n",
    "#             raise NotImplementedError()\n",
    "\n",
    "#         for pos in range(self.size):\n",
    "#             if pos == self.agent_position:\n",
    "#                 print(\"X\", end='')\n",
    "#             else:\n",
    "#                 print('.', end='')\n",
    "#             print('')\n",
    "\n",
    "    def reset(self):\n",
    "        # -1 to ensure agent inital position will not be at the end state\n",
    "        self.observe_idx = np.random.randint(self.size - 2)\n",
    "        self.agent_position = data[self.observe_idx]\n",
    "\n",
    "        return np.array([self.agent_position]).astype(np.uint8)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is to test if custom enviroment created properly\n",
    "# If the environment don't follow the gym interface, an error will be thrown\n",
    "\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "data = [0,9,7,4,3,5]\n",
    "\n",
    "env = Betting(data)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# try draw the grid world\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| explained_variance | -0.0147  |\n",
      "| fps                | 43       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| policy_loss        | 1.29     |\n",
      "| total_timesteps    | 0        |\n",
      "| value_loss         | 4.87     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.934    |\n",
      "| fps                | 1396     |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.547    |\n",
      "| policy_loss        | -0.207   |\n",
      "| total_timesteps    | 2079     |\n",
      "| value_loss         | 0.181    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.972    |\n",
      "| fps                | 1637     |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.6      |\n",
      "| policy_loss        | 0.0937   |\n",
      "| total_timesteps    | 4179     |\n",
      "| value_loss         | 0.0561   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.999    |\n",
      "| fps                | 1726     |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.503    |\n",
      "| policy_loss        | -0.004   |\n",
      "| total_timesteps    | 6279     |\n",
      "| value_loss         | 0.00105  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.999    |\n",
      "| fps                | 1764     |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.522    |\n",
      "| policy_loss        | -0.011   |\n",
      "| total_timesteps    | 8379     |\n",
      "| value_loss         | 0.000562 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 1        |\n",
      "| fps                | 1798     |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.529    |\n",
      "| policy_loss        | -0.0108  |\n",
      "| total_timesteps    | 10479    |\n",
      "| value_loss         | 0.00038  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import various RL algorithms\n",
    "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
    "\n",
    "# Train the agent\n",
    "model = ACKTR('MlpPolicy', env, verbose=1).learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  1\n",
      "obs= [4] reward= 1 done= False\n",
      "Step 2\n",
      "Action:  1\n",
      "obs= [3] reward= 1 done= False\n",
      "Step 3\n",
      "Action:  0\n",
      "obs= [3] reward= 1 done= True\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "# running the simulation with trained model to verify result\n",
    "\n",
    "obs = env.reset()\n",
    "n_steps = 50\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  0\n",
      "1  :  0\n",
      "2  :  0\n",
      "3  :  0\n",
      "4  :  1\n",
      "5  :  1\n",
      "6  :  1\n",
      "7  :  1\n",
      "8  :  1\n",
      "9  :  1\n"
     ]
    }
   ],
   "source": [
    "# print the trained policy map\n",
    "# recall UP = 0, DOWN = 1\n",
    "for i in range(10):\n",
    "    obs = [i]\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    print(i, \" : \", action, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf115",
   "language": "python",
   "name": "tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
